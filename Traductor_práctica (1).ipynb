{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **TRADUCCIÓN DE TEXTO**\n",
        "\n",
        "> Elena Gómez\\\n",
        "Ana Muñoz\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d9BYkL7rPtmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATOS**"
      ],
      "metadata": {
        "id": "KRJC8tHYKRha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este proyecto nuestro objetivo es poder crear un modelo generativo de texto que sea capaz de **traducir** texto de entrada en inglés, al español"
      ],
      "metadata": {
        "id": "uw4cBEIsRrN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encontramos un conjunto de datos en Kaggle que recoge biografías traducidas de Wikipedia del inglés al español.\\\n",
        "Link al dataset: \n",
        "https://www.kaggle.com/datasets/paultimothymooney/translated-wikipedia-biographies?select=Translated+Wikipedia+Biographies+-+EN_ES.csv"
      ],
      "metadata": {
        "id": "Z4kwAzdSSOj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "#keras.version_\n",
        "import pandas as pd\n",
        "df = pd.read_csv('datos.csv')"
      ],
      "metadata": {
        "id": "q4aD1s_aiOQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "VJLcA2DQmuDK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "3ad75f08-8d8c-415d-e750-491543432fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  sourceLanguage targetLanguage  documentID stringID  \\\n",
              "0             en             es           1      1-1   \n",
              "1             en             es           1      1-2   \n",
              "2             en             es           1      1-3   \n",
              "3             en             es           1      1-4   \n",
              "4             en             es           1      1-5   \n",
              "\n",
              "                                          sourceText  \\\n",
              "0  Kaisa-Leena Mäkäräinen (born 11 January 1983) ...   \n",
              "1  Outside sports, Mäkäräinen is currently studyi...   \n",
              "2  Her team coach is Jonne Kähkönen, while Jarmo ...   \n",
              "3  Mäkäräinen was originally a cross-country skie...   \n",
              "4     She started training for the biathlon in 2003.   \n",
              "\n",
              "                                      translatedText perceivedGender  \\\n",
              "0  Kaisa-Leena Mäkäräinen (nacida el 11 de enero ...          Female   \n",
              "1  Además de los deportes, estudia actualmente en...          Female   \n",
              "2  El entrenador de su equipo es Jonne Kähkönen, ...          Female   \n",
              "3  Mäkäräinen era originalmente esquiadora de cam...          Female   \n",
              "4        Comenzó a entrenar para el biatlón en 2003.          Female   \n",
              "\n",
              "         entityName                                          sourceURL  \n",
              "0  Kaisa Mäkäräinen  https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...  \n",
              "1  Kaisa Mäkäräinen  https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...  \n",
              "2  Kaisa Mäkäräinen  https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...  \n",
              "3  Kaisa Mäkäräinen  https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...  \n",
              "4  Kaisa Mäkäräinen  https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5fef2d45-aa30-4c19-8bfb-2beaf55cf74e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sourceLanguage</th>\n",
              "      <th>targetLanguage</th>\n",
              "      <th>documentID</th>\n",
              "      <th>stringID</th>\n",
              "      <th>sourceText</th>\n",
              "      <th>translatedText</th>\n",
              "      <th>perceivedGender</th>\n",
              "      <th>entityName</th>\n",
              "      <th>sourceURL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>1</td>\n",
              "      <td>1-1</td>\n",
              "      <td>Kaisa-Leena Mäkäräinen (born 11 January 1983) ...</td>\n",
              "      <td>Kaisa-Leena Mäkäräinen (nacida el 11 de enero ...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Kaisa Mäkäräinen</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>1</td>\n",
              "      <td>1-2</td>\n",
              "      <td>Outside sports, Mäkäräinen is currently studyi...</td>\n",
              "      <td>Además de los deportes, estudia actualmente en...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Kaisa Mäkäräinen</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>1</td>\n",
              "      <td>1-3</td>\n",
              "      <td>Her team coach is Jonne Kähkönen, while Jarmo ...</td>\n",
              "      <td>El entrenador de su equipo es Jonne Kähkönen, ...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Kaisa Mäkäräinen</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>1</td>\n",
              "      <td>1-4</td>\n",
              "      <td>Mäkäräinen was originally a cross-country skie...</td>\n",
              "      <td>Mäkäräinen era originalmente esquiadora de cam...</td>\n",
              "      <td>Female</td>\n",
              "      <td>Kaisa Mäkäräinen</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>en</td>\n",
              "      <td>es</td>\n",
              "      <td>1</td>\n",
              "      <td>1-5</td>\n",
              "      <td>She started training for the biathlon in 2003.</td>\n",
              "      <td>Comenzó a entrenar para el biatlón en 2003.</td>\n",
              "      <td>Female</td>\n",
              "      <td>Kaisa Mäkäräinen</td>\n",
              "      <td>https://en.wikipedia.org/wiki/Kaisa_M%C3%A4k%C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5fef2d45-aa30-4c19-8bfb-2beaf55cf74e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5fef2d45-aa30-4c19-8bfb-2beaf55cf74e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5fef2d45-aa30-4c19-8bfb-2beaf55cf74e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "De todo el dataset seleccionamos únicamente las columnas que son significativas para nuestro proyecto, estas son **sourceText** que contiene la biografía en inglés y **translatedText** donde se recoge su traducción"
      ],
      "metadata": {
        "id": "SHzhGRmhTLQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = df[['sourceText','translatedText']]"
      ],
      "metadata": {
        "id": "ENZrElSjm44w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "jgT0fOoFoaur",
        "outputId": "7cc33342-4050-49bb-bc02-0f10b09d0895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sourceText  \\\n",
              "0     Kaisa-Leena Mäkäräinen (born 11 January 1983) ...   \n",
              "1     Outside sports, Mäkäräinen is currently studyi...   \n",
              "2     Her team coach is Jonne Kähkönen, while Jarmo ...   \n",
              "3     Mäkäräinen was originally a cross-country skie...   \n",
              "4        She started training for the biathlon in 2003.   \n",
              "...                                                 ...   \n",
              "1466  Speaking to Madrid-based Diario AS in 2013 abo...   \n",
              "1467  Rossell proceeded to try again first under San...   \n",
              "1468  In the documentary \"Un Sueño Real\", she reveal...   \n",
              "1469                  Her struggle proved unsuccessful.   \n",
              "1470  It wasn't until 2013, in Perez's second stint ...   \n",
              "\n",
              "                                         translatedText  \n",
              "0     Kaisa-Leena Mäkäräinen (nacida el 11 de enero ...  \n",
              "1     Además de los deportes, estudia actualmente en...  \n",
              "2     El entrenador de su equipo es Jonne Kähkönen, ...  \n",
              "3     Mäkäräinen era originalmente esquiadora de cam...  \n",
              "4           Comenzó a entrenar para el biatlón en 2003.  \n",
              "...                                                 ...  \n",
              "1466  En 2013, habló de sus primeras frustraciones c...  \n",
              "1467  Rossell volvió a intentarlo con el sucesor de ...  \n",
              "1468  En el documental «Un sueño real», contó que se...  \n",
              "1469                            Su intento fue en vano.  \n",
              "1470  Recién en 2013, en el segundo mandato de Pérez...  \n",
              "\n",
              "[1471 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5c0fbe32-40db-4400-a2b5-2b9180825f49\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sourceText</th>\n",
              "      <th>translatedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kaisa-Leena Mäkäräinen (born 11 January 1983) ...</td>\n",
              "      <td>Kaisa-Leena Mäkäräinen (nacida el 11 de enero ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outside sports, Mäkäräinen is currently studyi...</td>\n",
              "      <td>Además de los deportes, estudia actualmente en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Her team coach is Jonne Kähkönen, while Jarmo ...</td>\n",
              "      <td>El entrenador de su equipo es Jonne Kähkönen, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mäkäräinen was originally a cross-country skie...</td>\n",
              "      <td>Mäkäräinen era originalmente esquiadora de cam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>She started training for the biathlon in 2003.</td>\n",
              "      <td>Comenzó a entrenar para el biatlón en 2003.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>Speaking to Madrid-based Diario AS in 2013 abo...</td>\n",
              "      <td>En 2013, habló de sus primeras frustraciones c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1467</th>\n",
              "      <td>Rossell proceeded to try again first under San...</td>\n",
              "      <td>Rossell volvió a intentarlo con el sucesor de ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>In the documentary \"Un Sueño Real\", she reveal...</td>\n",
              "      <td>En el documental «Un sueño real», contó que se...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>Her struggle proved unsuccessful.</td>\n",
              "      <td>Su intento fue en vano.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>It wasn't until 2013, in Perez's second stint ...</td>\n",
              "      <td>Recién en 2013, en el segundo mandato de Pérez...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1471 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5c0fbe32-40db-4400-a2b5-2b9180825f49')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5c0fbe32-40db-4400-a2b5-2b9180825f49 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5c0fbe32-40db-4400-a2b5-2b9180825f49');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procedemos ahora a añadirle los tokens **[start]** y **[end]** a los textos en español, así indicamos donde se inicia la secuencia y donde debe acabar para después traducir."
      ],
      "metadata": {
        "id": "0X7GsnxCTa-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1['translatedText'] = df1['translatedText'].apply(lambda x: \"[start] \" + x + \" [end]\")\n"
      ],
      "metadata": {
        "id": "ogQiADzTpd09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176f5a7e-5cb1-4041-d6f5-87c60f02dacc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-83d8202e2090>:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df1['translatedText'] = df1['translatedText'].apply(lambda x: \"[start] \" + x + \" [end]\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "iK2CzcEDfTIx",
        "outputId": "b75303f7-8578-4b57-a114-9cb8ecb6c001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sourceText  \\\n",
              "0     Kaisa-Leena Mäkäräinen (born 11 January 1983) ...   \n",
              "1     Outside sports, Mäkäräinen is currently studyi...   \n",
              "2     Her team coach is Jonne Kähkönen, while Jarmo ...   \n",
              "3     Mäkäräinen was originally a cross-country skie...   \n",
              "4        She started training for the biathlon in 2003.   \n",
              "...                                                 ...   \n",
              "1466  Speaking to Madrid-based Diario AS in 2013 abo...   \n",
              "1467  Rossell proceeded to try again first under San...   \n",
              "1468  In the documentary \"Un Sueño Real\", she reveal...   \n",
              "1469                  Her struggle proved unsuccessful.   \n",
              "1470  It wasn't until 2013, in Perez's second stint ...   \n",
              "\n",
              "                                         translatedText  \n",
              "0     [start] Kaisa-Leena Mäkäräinen (nacida el 11 d...  \n",
              "1     [start] Además de los deportes, estudia actual...  \n",
              "2     [start] El entrenador de su equipo es Jonne Kä...  \n",
              "3     [start] Mäkäräinen era originalmente esquiador...  \n",
              "4     [start] Comenzó a entrenar para el biatlón en ...  \n",
              "...                                                 ...  \n",
              "1466  [start] En 2013, habló de sus primeras frustra...  \n",
              "1467  [start] Rossell volvió a intentarlo con el suc...  \n",
              "1468  [start] En el documental «Un sueño real», cont...  \n",
              "1469              [start] Su intento fue en vano. [end]  \n",
              "1470  [start] Recién en 2013, en el segundo mandato ...  \n",
              "\n",
              "[1471 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1b02b71-f97e-40b9-8851-1f9fc519dd8e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sourceText</th>\n",
              "      <th>translatedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kaisa-Leena Mäkäräinen (born 11 January 1983) ...</td>\n",
              "      <td>[start] Kaisa-Leena Mäkäräinen (nacida el 11 d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Outside sports, Mäkäräinen is currently studyi...</td>\n",
              "      <td>[start] Además de los deportes, estudia actual...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Her team coach is Jonne Kähkönen, while Jarmo ...</td>\n",
              "      <td>[start] El entrenador de su equipo es Jonne Kä...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mäkäräinen was originally a cross-country skie...</td>\n",
              "      <td>[start] Mäkäräinen era originalmente esquiador...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>She started training for the biathlon in 2003.</td>\n",
              "      <td>[start] Comenzó a entrenar para el biatlón en ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>Speaking to Madrid-based Diario AS in 2013 abo...</td>\n",
              "      <td>[start] En 2013, habló de sus primeras frustra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1467</th>\n",
              "      <td>Rossell proceeded to try again first under San...</td>\n",
              "      <td>[start] Rossell volvió a intentarlo con el suc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>In the documentary \"Un Sueño Real\", she reveal...</td>\n",
              "      <td>[start] En el documental «Un sueño real», cont...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>Her struggle proved unsuccessful.</td>\n",
              "      <td>[start] Su intento fue en vano. [end]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>It wasn't until 2013, in Perez's second stint ...</td>\n",
              "      <td>[start] Recién en 2013, en el segundo mandato ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1471 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1b02b71-f97e-40b9-8851-1f9fc519dd8e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1b02b71-f97e-40b9-8851-1f9fc519dd8e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1b02b71-f97e-40b9-8851-1f9fc519dd8e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de hacer las divisiones para los subconjuntos de entrenamiento, validación y prueba, mezclamos los registros del dataset de forma aleatoria"
      ],
      "metadata": {
        "id": "GmHEU-WnWfNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mezclamos de forma aleatoria el dataset\n",
        "import random\n",
        "filas = list(df1.index)\n",
        "random.shuffle(filas)\n",
        "df1 = df1.loc[filas]"
      ],
      "metadata": {
        "id": "TEuRTlCefg9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez mezclado, dividimos el conjunto de forma que el conjunto de entrenamiento será el 70% del total, el de validación un 15% y el de test el 15% restante."
      ],
      "metadata": {
        "id": "4U3P_sjoWnZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15% validación\n",
        "num_val_samples = int(0.15 * len(df1))\n",
        "# 70% entrenamiento\n",
        "num_train_samples = len(df1) - 2 * num_val_samples\n",
        "\n",
        "train_pairs = df1[:num_train_samples]\n",
        "val_pairs = df1[num_train_samples:num_train_samples + num_val_samples]\n",
        "\n",
        "# 15% test o pruebas\n",
        "test_pairs = df1[num_train_samples + num_val_samples:]"
      ],
      "metadata": {
        "id": "tHXRMoXm3ohN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pairs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9lN2hZ1O88Jf",
        "outputId": "1cf505e5-7c2b-4ef4-9b89-9a73175d55f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sourceText  \\\n",
              "802   She has been a banking chief executive in her ...   \n",
              "594   Her Habilitation in philosophy was completed a...   \n",
              "151   As a professional dancer and choreographer, sh...   \n",
              "614   She has won multiple awards for her work on br...   \n",
              "226   He created over 40 works for his new company a...   \n",
              "...                                                 ...   \n",
              "806   Following education both in Zambia and abroad,...   \n",
              "1236  She was the first woman with this profession i...   \n",
              "1382  The band has been voted by fans across the wor...   \n",
              "1383  During their \"Best Xmas 2018 Concert\" at Akasa...   \n",
              "811   He is the former Vice-President of the African...   \n",
              "\n",
              "                                         translatedText  \n",
              "802   [start] Se ha desempeñado como directora ejecu...  \n",
              "594   [start] Obtuvo su habilitación en filosofía en...  \n",
              "151   [start] Como bailarina y coreógrafa profesiona...  \n",
              "614   [start] Recibió varios reconocimientos por su ...  \n",
              "226   [start] Ha creado más de 40 obras para su nuev...  \n",
              "...                                                 ...  \n",
              "806   [start] Después de formarse en Zambia y en el ...  \n",
              "1236  [start] Fue la primera mujer que ejerció esta ...  \n",
              "1382  [start] Sus seguidores de todo el mundo votaro...  \n",
              "1383  [start] Durante el «Best Xmas 2018 Concert» en...  \n",
              "811   [start] Fue vicepresidente del Banco de Desarr...  \n",
              "\n",
              "[1031 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-022b407b-7a6a-4d5f-b568-1df04f0d9053\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sourceText</th>\n",
              "      <th>translatedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>She has been a banking chief executive in her ...</td>\n",
              "      <td>[start] Se ha desempeñado como directora ejecu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>Her Habilitation in philosophy was completed a...</td>\n",
              "      <td>[start] Obtuvo su habilitación en filosofía en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>As a professional dancer and choreographer, sh...</td>\n",
              "      <td>[start] Como bailarina y coreógrafa profesiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614</th>\n",
              "      <td>She has won multiple awards for her work on br...</td>\n",
              "      <td>[start] Recibió varios reconocimientos por su ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>He created over 40 works for his new company a...</td>\n",
              "      <td>[start] Ha creado más de 40 obras para su nuev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>806</th>\n",
              "      <td>Following education both in Zambia and abroad,...</td>\n",
              "      <td>[start] Después de formarse en Zambia y en el ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>She was the first woman with this profession i...</td>\n",
              "      <td>[start] Fue la primera mujer que ejerció esta ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1382</th>\n",
              "      <td>The band has been voted by fans across the wor...</td>\n",
              "      <td>[start] Sus seguidores de todo el mundo votaro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1383</th>\n",
              "      <td>During their \"Best Xmas 2018 Concert\" at Akasa...</td>\n",
              "      <td>[start] Durante el «Best Xmas 2018 Concert» en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>He is the former Vice-President of the African...</td>\n",
              "      <td>[start] Fue vicepresidente del Banco de Desarr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1031 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-022b407b-7a6a-4d5f-b568-1df04f0d9053')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-022b407b-7a6a-4d5f-b568-1df04f0d9053 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-022b407b-7a6a-4d5f-b568-1df04f0d9053');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextVectorization"
      ],
      "metadata": {
        "id": "OBEjbKmWKYxB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora añadimos la capa de TextVectorization.\\\n",
        "Como la forma del texto en español es diferente a la del inglés, debemos preparar dos capas separadas, personalizadas para cada idioma.\n",
        "\n",
        "En cuanto al texto en español, debemos conservar los tokens añadidos anteriormente ([start] y [end]). Por lo tanto no podemos eliminar los caracteres \"[\", \"]\".Si no se especifica se eliminarían por defecto.\\\n",
        "Si se eliminasen, no podríamos distinguir la palabra \"start\" en inglés del token \"[start]\".\\\n",
        "En nuestro caso, dado que tenemos un conjunto de datos pequeño, facilitaremos el proceso eliminando los signos de puntuación, sería más eficaz y obtendríamos mejores resultados, por lo menos más realistas y coherentes, si entrenasemos un modelo con signos de puntuación incluidos.\n"
      ],
      "metadata": {
        "id": "qX9JV_sSXKpa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En general, a modo de resumen, lo que hacemos al aplicar la capa TextVectorization es:\n",
        "\n",
        "\n",
        "1.   Estandarizar, por ejemplo convertir todo el texto a minusculas y eliminar los signos de puntuación\n",
        "2.   Tokenización: dividir el texto en tokens o unidades\n",
        "3. Indexación: convertir cada token en un vector numérico\n",
        "\n"
      ],
      "metadata": {
        "id": "4aBvvmrNAVaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Para la capa TextVectorization en \n",
        "# español: conserva [ y ] pero elimina ¿ (así como \n",
        "# todos los demás caracteres de cadenas.puntuación)\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "print(strip_chars)\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "print(strip_chars)\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "print(strip_chars)\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "# Nos quedamos con las 15.000 palabras principales en cada idioma y \n",
        "# restringiremos las oraciones a 30 palabras.\n",
        "vocab_size = 15000\n",
        "sequence_length = 30\n",
        "\n",
        "# La capa en Inglés\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "# La capa en Español\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    # Generamos oraciones en español que tengan un token \n",
        "    # adicional, ya que necesitaremos compensar la oración \n",
        "    # en un paso durante el entrenamiento\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = train_pairs['sourceText']\n",
        "train_spanish_texts = train_pairs['translatedText']\n",
        "# Aprende el vocabulario de cada idioma\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF5VCFxK3oTH",
        "outputId": "a3cf3d6d-7e7d-4506-cd0f-3dfc0dd4db08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~¿\n",
            "!\"#$%&'()*+,-./:;<=>?@\\]^_`{|}~¿\n",
            "!\"#$%&'()*+,-./:;<=>?@\\^_`{|}~¿\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_spanish_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOu5TmQC7-Vg",
        "outputId": "9d17002b-d05e-45f5-c3f7-3d3625fe9a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "802     [start] Se ha desempeñado como directora ejecu...\n",
              "594     [start] Obtuvo su habilitación en filosofía en...\n",
              "151     [start] Como bailarina y coreógrafa profesiona...\n",
              "614     [start] Recibió varios reconocimientos por su ...\n",
              "226     [start] Ha creado más de 40 obras para su nuev...\n",
              "                              ...                        \n",
              "806     [start] Después de formarse en Zambia y en el ...\n",
              "1236    [start] Fue la primera mujer que ejerció esta ...\n",
              "1382    [start] Sus seguidores de todo el mundo votaro...\n",
              "1383    [start] Durante el «Best Xmas 2018 Concert» en...\n",
              "811     [start] Fue vicepresidente del Banco de Desarr...\n",
              "Name: translatedText, Length: 1031, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(random.choice(train_spanish_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_DLNaOf35p6",
        "outputId": "02ea7204-247a-4289-93bc-d15a58b5f7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[start] A partir de 1990, fue director del Centro Panruso de Cirugía Plástica y Oftálmica (Ufa). [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        # La oración de entrada en español \n",
        "        # no incluye el último token para \n",
        "        # mantener las entradas y los \n",
        "        # objetivos en la misma longitud.\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    # La frase objetivo en español está un \n",
        "    # paso por delante. Ambos siguen siendo \n",
        "    # de la misma longitud (20 palabras)\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts = pairs.iloc[:, 0]     #1ª columna (sourceText)\n",
        "    spa_texts = pairs.iloc[:, 1]     #2ª columna (translatedText)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    # Utilizamos el almacenamiento en caché en memoria \n",
        "    # para acelerar el preprocesamiento\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "Mef49Gdm4CLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Así es como se ven los resultados de nuestro conjunto de datos"
      ],
      "metadata": {
        "id": "jUdIks8Rbva6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unbb84zX4TfU",
        "outputId": "a5dadd4b-5dd7-4461-f370-2f3a54457997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs['english'].shape: (32, 30)\n",
            "inputs['spanish'].shape: (32, 30)\n",
            "targets.shape: (32, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FUNCIONES**"
      ],
      "metadata": {
        "id": "u3HaLoIj5XBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez tenemos los datos listos, vamos a definir e implementar varias funciones útiles y necesarias para nuestro modelo Transformer"
      ],
      "metadata": {
        "id": "8ORAbqqoRSYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positional Embedding"
      ],
      "metadata": {
        "id": "xEd4UpG3Kf9s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El Transformer es un enfoque agnóstico de orden, pero inyecta cierta información del orden manualmente en las representaciones que procesa, a esto lo conocemos como **Positional Encoding**\n",
        "\n",
        "Para dotar al modelo de la información del orden de las palabras, agregamos la posición de la palabra en la oración a cada word-embedding.\\\n",
        "Ahora, las word-embeddings tendrán dos componentes: el vector de palabra, que representa la palabra independiente del contexto, y un vector de posición, que indica la posición de la palabra en la oración actual.\n",
        "\n",
        "Aplicando **Positional Embeddings** (incrustación posicional) conseguimos aprender vectores de embedding de posición de la misma manera que aprendimos a incrustar los índices de palabras.\\\n",
        "Se agregan los embeddings de posición a las word-embeddings correspondientes, para así obtener una word-embedding consciente de la posición\n",
        "\n"
      ],
      "metadata": {
        "id": "dY038lf_JaGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ],
      "metadata": {
        "id": "c276RwrM4ahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Codificador y Decodificador"
      ],
      "metadata": {
        "id": "jM_tvWiFKj0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**CODIFICADOR TRANSFORMER**\\\n",
        "El codificador del Transformer lee la secuencia original y produce una representación de la misma codificada.\\\n",
        "Este codificador mantiene la representación en un formato de secuencia, es decir, una secuencia de vectores de embeddings conscientes del contexto.\n",
        "\n",
        "**DECODIFICADOR TRANSFORMER**\\\n",
        "Las partes internas del decodificador se parece mucho al codificador de Transformer, excepto que se inserta un bloque de attention adicional entre el bloque de self-attention aplicado a la secuencia de destino y las capas densas del bloque de salida."
      ],
      "metadata": {
        "id": "Sc09cgATfioQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzIqOhq60H5m"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Tamaño de los vectores de los tokens de entrada\n",
        "        self.embed_dim = embed_dim\n",
        "        # Tamaño de la capa densa interna\n",
        "        self.dense_dim = dense_dim\n",
        "        # Número de attention heads\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    # El cálculo va en call()\n",
        "    def call(self, inputs, mask=None):\n",
        "        # La máscara que generará la capa Embedding \n",
        "        # será 2D, pero la capa de atención espera \n",
        "        # ser 3D o 4D, por lo que ampliamos su rango\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    # Implementamos la serialización para \n",
        "    # que podamos guardar el modelo\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La capa **``LayerNormalization``** normaliza cada secuencia independientemente de otras secuencias en el lote, es decir, agrupa los datos dentro de cada secuencia por separadoa."
      ],
      "metadata": {
        "id": "aZo1w-C6RZbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de la entrada: (batch_size, sequence_length, embedding_dim)\n",
        "def layer_normalization(batch_of_sequences):\n",
        "    # Para calcular la media y la varianza, solo \n",
        "    # agrupamos datos sobre el último eje (eje -1)\n",
        "    mean = np.mean(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    variance = np.var(batch_of_sequences, keepdims=True, axis=-1)\n",
        "    return (batch_of_sequences - mean) / variance"
      ],
      "metadata": {
        "id": "V8AfbC1CLw4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "Y29YpdF15lj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forma de la entrada: (batch_size, height, width, channels)\n",
        "def batch_normalization(batch_of_images):\n",
        "    # Agrupa los datos sobre el eje del lote (eje 0), \n",
        "    # lo que crea interacciones entre las muestras en un lote.\n",
        "    mean = np.mean(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "    variance = np.var(batch_of_images, keepdims=True, axis=(0, 1, 2))\n",
        "    return (batch_of_images - mean) / variance"
      ],
      "metadata": {
        "id": "zqCIRH0YMUW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AYrZ3220RW4"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        # Este atributo asegura que la capa propagará \n",
        "        # su máscara de entrada a sus salidas; el \n",
        "        # enmascaramiento en Keras es explícitamente \n",
        "        # opt-in. Si pasa una máscara a una capa que \n",
        "        # no implementa compute_mask() y que no expone \n",
        "        # este atributo support_masking, es un error.\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention**"
      ],
      "metadata": {
        "id": "pOyHXczDKq4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Procedemos a añadir el enmascaramiento a la mitad superior de la matriz de atención para evitar que el modelo prese atención a la información del futuro, ya que solo debe usar la información del token 0 hasta el N para generar el token N+1.\\\n",
        "Para hacer esto, agregaremos un método **get_causal_attention_mask(self, inputs)** a nuestro TransformerDecoder para recuperar una máscara de atención que podemos pasar a nuestras capas MultiHeadAttention."
      ],
      "metadata": {
        "id": "kznOjT2xgqBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        # Genere una matriz de forma (sequence_length, sequence_length) \n",
        "        # con 1 en una mitad y 0 en la otra\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        # Lo replicamos a lo largo del eje del lote para obtener una matriz \n",
        "        # de forma  (batch_size, sequence_length, sequence_length)\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)"
      ],
      "metadata": {
        "id": "bND78dATREAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call(self, inputs, encoder_outputs, mask=None):\n",
        "        # Recupera la máscara causal\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        # Prepara la máscara de entrada (que describe las \n",
        "        # ubicaciones de relleno en la secuencia de destino)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            # Fusiona las dos máscaras juntas\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            # Pasamos la máscara causal a la primera capa de atención, \n",
        "            # que realiza la self-attention sobre la secuencia de destino.\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            # Pasamos la máscara combinada a la segunda \n",
        "            # capa de atención, que relaciona la secuencia \n",
        "            # de origen con la secuencia de destino\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ],
      "metadata": {
        "id": "jBxJmTwySHp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwdFFlly0RW4"
      },
      "source": [
        "# **Transformer Extremo-a-Extremo**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el objetivo que tenemos, decidimos utilizar un Transformer ya que es lo más adecuado en el aprendizaje de secuencia-a-secuencia.\\\n",
        "La atención neuronal que usan estos modelos, les permite procesar con éxito secuencias más largas y complejas que las que se manejan en los RNN.\n",
        "\n",
        "Nosotros, cuando traducimos texto no leemos el texto palabra por palabra, manteniendo su significado y generando la traducción palabra a palabra en español.\\\n",
        "Lo que hacemos es alternar entre la oración original y la traducción simultaneamente, prestando atención al contexto y a las diferentes palabras.\\\n",
        "Eso es exactamente lo que podemos lograr con atención neuronal y Transformers."
      ],
      "metadata": {
        "id": "fsBcBKZp-t2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya tenemos todo listo para montar el Transformer y entrenarlo"
      ],
      "metadata": {
        "id": "X4r_nb4ZiB2q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3EiJtWk0RW4"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "dense_dim = 128\n",
        "num_heads = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "# Codificamos la oración fuente\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "# Codificamos la oración objetivo y la combinamos con la oración fuente codificada\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predecimos una palabra para cada posición de salida\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkMCBUmr0RW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b629055-b839-4178-e0d1-3fd6998964b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "33/33 [==============================] - 61s 2s/step - loss: 5.9637 - accuracy: 0.1963 - val_loss: 5.8579 - val_accuracy: 0.2155\n",
            "Epoch 2/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 5.5316 - accuracy: 0.2338 - val_loss: 5.7010 - val_accuracy: 0.2270\n",
            "Epoch 3/30\n",
            "33/33 [==============================] - 41s 1s/step - loss: 5.1791 - accuracy: 0.2642 - val_loss: 5.6063 - val_accuracy: 0.2462\n",
            "Epoch 4/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 4.8593 - accuracy: 0.2931 - val_loss: 5.5489 - val_accuracy: 0.2366\n",
            "Epoch 5/30\n",
            "33/33 [==============================] - 48s 1s/step - loss: 4.5745 - accuracy: 0.3190 - val_loss: 5.5006 - val_accuracy: 0.2452\n",
            "Epoch 6/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 4.3090 - accuracy: 0.3453 - val_loss: 5.4364 - val_accuracy: 0.2532\n",
            "Epoch 7/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 4.0631 - accuracy: 0.3695 - val_loss: 5.4389 - val_accuracy: 0.2507\n",
            "Epoch 8/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 3.8142 - accuracy: 0.3909 - val_loss: 5.6309 - val_accuracy: 0.2378\n",
            "Epoch 9/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 3.5711 - accuracy: 0.4189 - val_loss: 5.4202 - val_accuracy: 0.2605\n",
            "Epoch 10/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 3.3483 - accuracy: 0.4473 - val_loss: 5.4932 - val_accuracy: 0.2648\n",
            "Epoch 11/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 3.1002 - accuracy: 0.4830 - val_loss: 5.4918 - val_accuracy: 0.2788\n",
            "Epoch 12/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 2.8597 - accuracy: 0.5172 - val_loss: 5.5371 - val_accuracy: 0.2779\n",
            "Epoch 13/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 2.6283 - accuracy: 0.5503 - val_loss: 5.6968 - val_accuracy: 0.2577\n",
            "Epoch 14/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 2.4008 - accuracy: 0.5884 - val_loss: 5.5935 - val_accuracy: 0.2710\n",
            "Epoch 15/30\n",
            "33/33 [==============================] - 49s 1s/step - loss: 2.1582 - accuracy: 0.6283 - val_loss: 5.6124 - val_accuracy: 0.2702\n",
            "Epoch 16/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 1.9593 - accuracy: 0.6584 - val_loss: 5.6175 - val_accuracy: 0.2732\n",
            "Epoch 17/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 1.7584 - accuracy: 0.6896 - val_loss: 5.7283 - val_accuracy: 0.2673\n",
            "Epoch 18/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 1.5660 - accuracy: 0.7238 - val_loss: 5.6956 - val_accuracy: 0.2773\n",
            "Epoch 19/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 1.4059 - accuracy: 0.7482 - val_loss: 5.8336 - val_accuracy: 0.2706\n",
            "Epoch 20/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 1.2363 - accuracy: 0.7797 - val_loss: 5.8194 - val_accuracy: 0.2650\n",
            "Epoch 21/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 1.0849 - accuracy: 0.8053 - val_loss: 5.8242 - val_accuracy: 0.2777\n",
            "Epoch 22/30\n",
            "33/33 [==============================] - 44s 1s/step - loss: 0.9669 - accuracy: 0.8315 - val_loss: 5.9349 - val_accuracy: 0.2773\n",
            "Epoch 23/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 0.8545 - accuracy: 0.8537 - val_loss: 5.9963 - val_accuracy: 0.2603\n",
            "Epoch 24/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 0.7518 - accuracy: 0.8747 - val_loss: 6.0932 - val_accuracy: 0.2765\n",
            "Epoch 25/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 0.6492 - accuracy: 0.8944 - val_loss: 6.1219 - val_accuracy: 0.2704\n",
            "Epoch 26/30\n",
            "33/33 [==============================] - 49s 1s/step - loss: 0.5681 - accuracy: 0.9111 - val_loss: 6.1617 - val_accuracy: 0.2753\n",
            "Epoch 27/30\n",
            "33/33 [==============================] - 44s 1s/step - loss: 0.4987 - accuracy: 0.9237 - val_loss: 6.1458 - val_accuracy: 0.2618\n",
            "Epoch 28/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 0.4272 - accuracy: 0.9389 - val_loss: 6.2219 - val_accuracy: 0.2704\n",
            "Epoch 29/30\n",
            "33/33 [==============================] - 43s 1s/step - loss: 0.3692 - accuracy: 0.9478 - val_loss: 6.2682 - val_accuracy: 0.2710\n",
            "Epoch 30/30\n",
            "33/33 [==============================] - 42s 1s/step - loss: 0.3397 - accuracy: 0.9505 - val_loss: 6.3291 - val_accuracy: 0.2681\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdceb601ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos que conseguimos, para los datos de validación, unos valores de precisión bastante bajos para nuestro modelo.\\\n",
        "Esto se puede deber al hecho de contar con un dataset pequeño y en el que cada oración trata un tema diferente"
      ],
      "metadata": {
        "id": "HbvlWv2IQsW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traducimos nuevas oraciones en inglés nunca antes vistas con nuestro modelo"
      ],
      "metadata": {
        "id": "prGv80AUQ3lz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3H2505-0RW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90fe511a-6ce0-412f-f9f3-e63c5251d601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "1125    British troops occupied the city on 6 April.\n",
            "Name: sourceText, dtype: object\n",
            "[start] este se dedicó a la ciudad natal en el condado de abril de harvard [end]\n",
            "-\n",
            "1110    In 1971, Douglas became a foundation lecturer ...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en 1971 llegó a ser profesora de bibliotecología de estudios de bibliotecología de bibliotecología de la universidad de la universidad de las indias occidentales donde se fijó en la universidad\n",
            "-\n",
            "579    She has also been accused of apostasy which is...\n",
            "Name: sourceText, dtype: object\n",
            "[start] tiene más conocido como el caso de papua new guinea lo que pidieran [end]\n",
            "-\n",
            "1228    Rachel Mary Parsons was born in 1885, to Sir C...\n",
            "Name: sourceText, dtype: object\n",
            "[start] rachel mary parsons 1885–1956 ingeniera en el bank of the town donde abordó las políticas principales cirujanas de la hija mayor de la hija de sangre en nepal y seguridad\n",
            "-\n",
            "548    Wael Younis (Arabic: وائل يونس‎, Hebrew: ואאל ...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en el 1 de cody fueron ingenieros en el 31 de 1963 es un salto de los juegos olímpicos de plata en el contraataque y en el campo de la\n",
            "-\n",
            "780    Her paternal grandfather and his brother found...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en 1978 después de su hermano y parte de muerte en 1991 lanzó el área científica jefa en el tercer lugar en la entrenó empleando el apodo de anambra [end]\n",
            "-\n",
            "809    She was also the first woman CEO in all of Bar...\n",
            "Name: sourceText, dtype: object\n",
            "[start] fue directora ejecutiva de la primera vez en el primer iraní en lagos nigeria ante el campeonato europeo [end]\n",
            "-\n",
            "1353    By 2017, BTS crossed into the global music mar...\n",
            "Name: sourceText, dtype: object\n",
            "[start] mientras 2017 trabajó en bremen [end]\n",
            "-\n",
            "579    She has also been accused of apostasy which is...\n",
            "Name: sourceText, dtype: object\n",
            "[start] tiene más conocido como el caso de papua new guinea lo que pidieran [end]\n",
            "-\n",
            "499    She was born in Duvauchelle, Banks Peninsula, ...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en 2014 nació en noviembre de febrero de nueva zelanda en lagos y en pista larga en el campeonato mundial [end]\n",
            "-\n",
            "505    In the suburb of Blockhouse Bay, Auckland, the...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en el campeonato mundial de febrero de 1980 es un park un ingeniero de la enfermería de la enfermería [end]\n",
            "-\n",
            "994    Park is leading the efforts to combat the spre...\n",
            "Name: sourceText, dtype: object\n",
            "[start] park es hijo único de 991 segundos para el santuario de febrero de kenya airways [end]\n",
            "-\n",
            "1113    Alia Muhammad Baker (also spelled \"Baqer\") was...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en 2013 ter mors había ganado dos santos 3 bibliotecario de bibliotecario adjunto en el estado de edimburgo [end]\n",
            "-\n",
            "261    After dancing for Martha Graham, he attended J...\n",
            "Name: sourceText, dtype: object\n",
            "[start] después de la danza decidió no podía pagar y en la escuela de ballet [end]\n",
            "-\n",
            "1353    By 2017, BTS crossed into the global music mar...\n",
            "Name: sourceText, dtype: object\n",
            "[start] mientras 2017 trabajó en bremen [end]\n",
            "-\n",
            "796    Margaret Mhango Mwanakatwe is a Zambian politi...\n",
            "Name: sourceText, dtype: object\n",
            "[start] margaret williamson menzies campbell fds frcse de la que fue el ministro de ser el febrero de 2018 y julio de julio de julio de julio de 2018 [end]\n",
            "-\n",
            "963    Following this she was employed as a social wo...\n",
            "Name: sourceText, dtype: object\n",
            "[start] después de esto fue el caso de las áreas de 1893 hija de la sociedad del departamento de las naciones unidas en el tiempo de las fuerzas de las fuerzas\n",
            "-\n",
            "1114    Baker saved an estimated 30,000 books from des...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en el 1 de 2018 concert» en el caso de odisha en el hospital neckerenfants malades de santander uk puesto [end]\n",
            "-\n",
            "737    In October 2014, she was named \"Business Leade...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en octubre de 2014 fue la organización de estableció de octubre de antes de estudios de cuba momento el área de nueva zelanda en el área de nigeria ante la\n",
            "-\n",
            "672    The plant Petagnaea gussonei has been named af...\n",
            "Name: sourceText, dtype: object\n",
            "[start] el inmunidad de febrero de huawei entre las plantas [end]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 30\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        # Muestra el siguiente token\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        # Convertimos la siguiente predicción del token en una \n",
        "        # cadena y la agregamos a la oración generada\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        # Condición de salida\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = test_pairs['sourceText']\n",
        "for _ in range(20):\n",
        "    input_sentence = test_eng_texts.sample(n=1)\n",
        "\n",
        "    #imprimir fila seleccionada\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1['sourceText'][152]"
      ],
      "metadata": {
        "id": "q4sPFAs9kBwd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c50f8866-5e7d-4877-a0b5-a5b1b4f7e75b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'She also choreographed The Channel O Music Video Awards.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tras ver los valores obtenidos al entrenar el transformer, no nos sorprende el resultado al utilizar este para traducir nuevas oraciones, ya que, como se puede apreciar, las traducciones son bastante incompletas."
      ],
      "metadata": {
        "id": "zJA-6FBIQ8CH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apilamos Encoder y Decoder"
      ],
      "metadata": {
        "id": "qZpPWJthoaN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A veces es útil apilar varias capas una tras otra para aumentar el poder de representación de una red. En tal configuración, debemos obtener todas las capas\n",
        "intermedias para devolver una secuencia completa de salidas.\\\n",
        "Lo hacemos indicando **return_sequences=True**"
      ],
      "metadata": {
        "id": "WCYUJhM0NMBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debido a los resultados anteriores, decidimos crear un nuevo modelo en el que vamos a apilar varias capas tanto del codificador como del decodificador, para buscar mejores resultados"
      ],
      "metadata": {
        "id": "MQ511OrKQfJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=64)(inputs)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "xneVyjRCYZnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 128\n",
        "num_heads = 2\n",
        "\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "# Codificamos la oración fuente\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(encoder_outputs)\n",
        "\n",
        "\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "# Deodificamos la oración objetivo y la combinamos con la oración fuente codificada\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predecimos una palabra para cada posición de salida\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "dxP-B1CXoZs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQqt7PjQq6It",
        "outputId": "d5112f1a-80e9-4962-d46b-6b86f94b63aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "33/33 [==============================] - 64s 2s/step - loss: 7.4274 - accuracy: 0.0967 - val_loss: 6.6250 - val_accuracy: 0.1275\n",
            "Epoch 2/30\n",
            "33/33 [==============================] - 54s 2s/step - loss: 6.4015 - accuracy: 0.1593 - val_loss: 6.2005 - val_accuracy: 0.1830\n",
            "Epoch 3/30\n",
            "33/33 [==============================] - 58s 2s/step - loss: 5.9495 - accuracy: 0.1968 - val_loss: 5.8958 - val_accuracy: 0.2075\n",
            "Epoch 4/30\n",
            "33/33 [==============================] - 62s 2s/step - loss: 5.5778 - accuracy: 0.2263 - val_loss: 5.7841 - val_accuracy: 0.2292\n",
            "Epoch 5/30\n",
            "33/33 [==============================] - 52s 2s/step - loss: 5.2307 - accuracy: 0.2611 - val_loss: 5.6341 - val_accuracy: 0.2370\n",
            "Epoch 6/30\n",
            "33/33 [==============================] - 54s 2s/step - loss: 4.9295 - accuracy: 0.2851 - val_loss: 5.5801 - val_accuracy: 0.2436\n",
            "Epoch 7/30\n",
            "33/33 [==============================] - 58s 2s/step - loss: 4.6259 - accuracy: 0.3165 - val_loss: 5.5370 - val_accuracy: 0.2354\n",
            "Epoch 8/30\n",
            "33/33 [==============================] - 52s 2s/step - loss: 4.3700 - accuracy: 0.3395 - val_loss: 5.5154 - val_accuracy: 0.2460\n",
            "Epoch 9/30\n",
            "33/33 [==============================] - 52s 2s/step - loss: 4.0928 - accuracy: 0.3672 - val_loss: 5.5958 - val_accuracy: 0.2409\n",
            "Epoch 10/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 3.8732 - accuracy: 0.3906 - val_loss: 5.5537 - val_accuracy: 0.2540\n",
            "Epoch 11/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 3.6231 - accuracy: 0.4165 - val_loss: 5.4855 - val_accuracy: 0.2548\n",
            "Epoch 12/30\n",
            "33/33 [==============================] - 58s 2s/step - loss: 3.3931 - accuracy: 0.4444 - val_loss: 5.5950 - val_accuracy: 0.2571\n",
            "Epoch 13/30\n",
            "33/33 [==============================] - 52s 2s/step - loss: 3.1354 - accuracy: 0.4797 - val_loss: 5.6550 - val_accuracy: 0.2575\n",
            "Epoch 14/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 2.9151 - accuracy: 0.5127 - val_loss: 5.6100 - val_accuracy: 0.2626\n",
            "Epoch 15/30\n",
            "33/33 [==============================] - 57s 2s/step - loss: 2.6800 - accuracy: 0.5462 - val_loss: 5.6190 - val_accuracy: 0.2716\n",
            "Epoch 16/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 2.4436 - accuracy: 0.5811 - val_loss: 5.6417 - val_accuracy: 0.2753\n",
            "Epoch 17/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 2.2231 - accuracy: 0.6210 - val_loss: 5.7340 - val_accuracy: 0.2601\n",
            "Epoch 18/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 2.0162 - accuracy: 0.6520 - val_loss: 5.7388 - val_accuracy: 0.2671\n",
            "Epoch 19/30\n",
            "33/33 [==============================] - 54s 2s/step - loss: 1.8029 - accuracy: 0.6926 - val_loss: 5.8458 - val_accuracy: 0.2612\n",
            "Epoch 20/30\n",
            "33/33 [==============================] - 62s 2s/step - loss: 1.6203 - accuracy: 0.7223 - val_loss: 5.8213 - val_accuracy: 0.2740\n",
            "Epoch 21/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 1.4469 - accuracy: 0.7503 - val_loss: 5.9296 - val_accuracy: 0.2685\n",
            "Epoch 22/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 1.2761 - accuracy: 0.7807 - val_loss: 6.0050 - val_accuracy: 0.2669\n",
            "Epoch 23/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 1.1249 - accuracy: 0.8082 - val_loss: 5.9625 - val_accuracy: 0.2646\n",
            "Epoch 24/30\n",
            "33/33 [==============================] - 57s 2s/step - loss: 0.9917 - accuracy: 0.8337 - val_loss: 6.0072 - val_accuracy: 0.2728\n",
            "Epoch 25/30\n",
            "33/33 [==============================] - 54s 2s/step - loss: 0.8600 - accuracy: 0.8579 - val_loss: 6.0589 - val_accuracy: 0.2716\n",
            "Epoch 26/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 0.7327 - accuracy: 0.8845 - val_loss: 6.1415 - val_accuracy: 0.2652\n",
            "Epoch 27/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 0.6396 - accuracy: 0.9050 - val_loss: 6.2652 - val_accuracy: 0.2589\n",
            "Epoch 28/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 0.5436 - accuracy: 0.9230 - val_loss: 6.2701 - val_accuracy: 0.2515\n",
            "Epoch 29/30\n",
            "33/33 [==============================] - 58s 2s/step - loss: 0.4659 - accuracy: 0.9368 - val_loss: 6.2980 - val_accuracy: 0.2689\n",
            "Epoch 30/30\n",
            "33/33 [==============================] - 53s 2s/step - loss: 0.3847 - accuracy: 0.9523 - val_loss: 6.3413 - val_accuracy: 0.2681\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcea8a71f0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos comprobar que con este modelo no conseguimos mejorar los valores de pérdida y precisión, obtenemos unos valores similares"
      ],
      "metadata": {
        "id": "sz36dLGbQYLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "qU_5kDxbQBCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Traducimos nuevas oraciones con el modelo ya entrenado"
      ],
      "metadata": {
        "id": "OytmX2IcQUEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 30\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "        # Muestra el siguiente token\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        # Convertimos la siguiente predicción del token en una \n",
        "        # cadena y la agregamos a la oración generada\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        # Condición de salida\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = test_pairs['sourceText']\n",
        "\n",
        "input_sec = []\n",
        "decode = []\n",
        "for _ in range(20):\n",
        "    input_sentence = test_eng_texts.sample(n=1)\n",
        "    input_sec.append(input_sentence)\n",
        "    decode.append(decode_sequence(input_sentence))\n",
        "    #imprimir fila seleccionada\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ],
      "metadata": {
        "id": "EzlQk2YgSzIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db11194-2e58-4356-845a-da459be133c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "1218    As of 2016, she is listed as the 38th most pow...\n",
            "Name: sourceText, dtype: object\n",
            "[start] como de 2016 figuró en la como auxiliar en las mujeres más poderosas del mundo según forbes [end]\n",
            "-\n",
            "897    In 1975 he was the first Papua New Guinean to ...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en 1975 obtuvo la licenciatura en papúa nueva guinea donde se convirtió en 1926 del programa de una carta abierta solicitando un nuevo sistema de papúa nueva york [end]\n",
            "-\n",
            "1399    In its beginning, the band combined reggae and...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en sus padres se ha ganado una familia de las áreas de rock japonesa de adultos servicios multiculturales y la ciudad [end]\n",
            "-\n",
            "814    He also undertook research work on the anticip...\n",
            "Name: sourceText, dtype: object\n",
            "[start] se llegó a investigación de investigación se incorporó al convento st hilda y un grupo emprendió en la universidad de adelaida donde se graduó de África occidental en la universidad\n",
            "-\n",
            "47    Improving in her new event, she had a best of ...\n",
            "Name: sourceText, dtype: object\n",
            "[start] bjørn ha obtuvo una licenciatura en la universidad de nueva york [end]\n",
            "-\n",
            "888    Mekere Morauta is also the most important oppo...\n",
            "Name: sourceText, dtype: object\n",
            "[start] nació en 1952 es una organización panamericana de la jugadora y política sanitaria en el condado de ellas [end]\n",
            "-\n",
            "230    Since then he has created Eternal Light and Th...\n",
            "Name: sourceText, dtype: object\n",
            "[start] desde principios de la universidad mcgill donde cursó estudios de irlanda galway antes [end]\n",
            "-\n",
            "1218    As of 2016, she is listed as the 38th most pow...\n",
            "Name: sourceText, dtype: object\n",
            "[start] como de 2016 figuró en la como auxiliar en las mujeres más poderosas del mundo según forbes [end]\n",
            "-\n",
            "316    Since then, her team has done 47 liver transpl...\n",
            "Name: sourceText, dtype: object\n",
            "[start] desde principios de la se ha sido el hospital arreglando las indias occidentales [end]\n",
            "-\n",
            "709    He then left the University to become Presiden...\n",
            "Name: sourceText, dtype: object\n",
            "[start] se hizo la facultad de medicina en la universidad de la universidad de otago entre 1980 [end]\n",
            "-\n",
            "548    Wael Younis (Arabic: وائل يونس‎, Hebrew: ואאל ...\n",
            "Name: sourceText, dtype: object\n",
            "[start] nació en la universidad de una familia de 1946 en enero de 1963 es un biólogo médico y cirugía cardiovascular [end]\n",
            "-\n",
            "995    Park studied the issue of poverty and social w...\n",
            "Name: sourceText, dtype: object\n",
            "[start] park se dedicó a la escuela de bibliotecología y también para tres años empezó a francia [end]\n",
            "-\n",
            "966    Ruth-Rolland was the president of the Central ...\n",
            "Name: sourceText, dtype: object\n",
            "[start] ruthrolland fue la primera mujer de la asociación japonesa de la india 20162018 [end]\n",
            "-\n",
            "1356    Formed in 1986, the duo became an internationa...\n",
            "Name: sourceText, dtype: object\n",
            "[start] se formó en 1986 es una banda de las fuerzas de la década de 1970 y 1980 en el álbum de la década de la década de 2000 fue un\n",
            "-\n",
            "287    Also an excellent table tennis player, she won...\n",
            "Name: sourceText, dtype: object\n",
            "[start] también había completado una licenciatura en la universidad de oro en el campeonato mundial obtuvo una medalla de las naciones unidas en 1963 actualizada en 1963 se desempeñó como consultor\n",
            "-\n",
            "916    Following an untimely demise of her mother, sh...\n",
            "Name: sourceText, dtype: object\n",
            "[start] después de una familia de una institución de la en la universidad de tel aviv [end]\n",
            "-\n",
            "109    During that season, in a game against the Quad...\n",
            "Name: sourceText, dtype: object\n",
            "[start] durante la temporada de la temporada de la temporada de la ciudad de marzo de 2014 a castonguay en 2008 avanzó a la euroliga y la ciudad rusa de educación\n",
            "-\n",
            "925    In recognition to her contribution to educatio...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en reconocimiento a su contribución a la forma en las indias occidentales donde se dos ocasiones la logró un área de la docencia y estuvo a los 12 años de\n",
            "-\n",
            "929    Glinka was born in Moscow.\n",
            "Name: sourceText, dtype: object\n",
            "[start] falleció nació en el 1 [end]\n",
            "-\n",
            "391    In 1984 he spent a year at Duke University Med...\n",
            "Name: sourceText, dtype: object\n",
            "[start] en 1984 se convirtió en el año obtuvo una licenciatura en la universidad de china y programa de china y de adelaida donde completó el año [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "u3ypyMDWUXNH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "8c16f718-7d69-44b4-e1cc-09b2ab5e4c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             sourceText  \\\n",
              "802   She has been a banking chief executive in her ...   \n",
              "594   Her Habilitation in philosophy was completed a...   \n",
              "151   As a professional dancer and choreographer, sh...   \n",
              "614   She has won multiple awards for her work on br...   \n",
              "226   He created over 40 works for his new company a...   \n",
              "...                                                 ...   \n",
              "245   His great-grandfather was a poet and a busines...   \n",
              "966   Ruth-Rolland was the president of the Central ...   \n",
              "1279  O'Kane returned to Australia, where she worked...   \n",
              "839   Later in 1997, Ármannsson was appointed the CE...   \n",
              "206   In 1980 he attended Varna International Ballet...   \n",
              "\n",
              "                                         translatedText  \n",
              "802   [start] Se ha desempeñado como directora ejecu...  \n",
              "594   [start] Obtuvo su habilitación en filosofía en...  \n",
              "151   [start] Como bailarina y coreógrafa profesiona...  \n",
              "614   [start] Recibió varios reconocimientos por su ...  \n",
              "226   [start] Ha creado más de 40 obras para su nuev...  \n",
              "...                                                 ...  \n",
              "245   [start] Su bisabuelo fue poeta y hombre de neg...  \n",
              "966   [start] Fue presidenta de la Cruz Roja Centroa...  \n",
              "1279  [start] En 1982, completó su doctorado. Regres...  \n",
              "839   [start] Luego, en 1997, Ármannsson fue designa...  \n",
              "206   [start] En 1980 asistió al Concurso Internacio...  \n",
              "\n",
              "[1471 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2da13d7e-9ea9-41a8-9103-46c978d7e09a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sourceText</th>\n",
              "      <th>translatedText</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>She has been a banking chief executive in her ...</td>\n",
              "      <td>[start] Se ha desempeñado como directora ejecu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>Her Habilitation in philosophy was completed a...</td>\n",
              "      <td>[start] Obtuvo su habilitación en filosofía en...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>As a professional dancer and choreographer, sh...</td>\n",
              "      <td>[start] Como bailarina y coreógrafa profesiona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614</th>\n",
              "      <td>She has won multiple awards for her work on br...</td>\n",
              "      <td>[start] Recibió varios reconocimientos por su ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>He created over 40 works for his new company a...</td>\n",
              "      <td>[start] Ha creado más de 40 obras para su nuev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>His great-grandfather was a poet and a busines...</td>\n",
              "      <td>[start] Su bisabuelo fue poeta y hombre de neg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>966</th>\n",
              "      <td>Ruth-Rolland was the president of the Central ...</td>\n",
              "      <td>[start] Fue presidenta de la Cruz Roja Centroa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1279</th>\n",
              "      <td>O'Kane returned to Australia, where she worked...</td>\n",
              "      <td>[start] En 1982, completó su doctorado. Regres...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>839</th>\n",
              "      <td>Later in 1997, Ármannsson was appointed the CE...</td>\n",
              "      <td>[start] Luego, en 1997, Ármannsson fue designa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>In 1980 he attended Varna International Ballet...</td>\n",
              "      <td>[start] En 1980 asistió al Concurso Internacio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1471 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2da13d7e-9ea9-41a8-9103-46c978d7e09a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2da13d7e-9ea9-41a8-9103-46c978d7e09a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2da13d7e-9ea9-41a8-9103-46c978d7e09a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def contar_palabras(fila):\n",
        "    return len(str(fila).split())\n",
        "\n",
        "media_palabras = df1['sourceText'].apply(contar_palabras).mean()\n",
        "print(media_palabras)\n",
        "media_palabras2 = df1['translatedText'].apply(contar_palabras).mean()\n",
        "print(media_palabras2)"
      ],
      "metadata": {
        "id": "Cxjf2czLUJ1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33eb147a-0f30-44f5-b62e-77a0d83dc2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20.70904146838885\n",
            "24.936777702243372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusiones del Transformer**\n",
        "\n",
        "La máxima precisión que logramos es un 27%, a priori diríamos que no es una modelo últil. La realidad es que el conjunto de datos contaba con tan solo 1471 registros, y el modelo fue entrenado con el 70%, es decir, hemos entrenado este modelo para traducir textos con solo 1029 pares de textos en inglés y español.\\\n",
        "En nuestra opinión, consideramos que consigue un muy buen resultado para los pocos datos de los que disponía.\n",
        "\n",
        "Además, hay que tener en cuenta que los datos no eran de un tema concreto, esto afecta a la hora del vocabulario. Al tratar de temas diferentes, las palabras y el contexto de estas no se repite tanto, por lo que suma dificultad.\n",
        "Por otro lado, como hemos podido ver, los textos originales son bastante largos, cosa que debemos tener en cuenta para evaluar el modelo."
      ],
      "metadata": {
        "id": "YKVMnXjFlnHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, nos interesa probar si con un modelo de redes neuronales recurrentes bidireccional conseguimos unos mejores resultados para los datos que tenemos."
      ],
      "metadata": {
        "id": "65U-vdsum4oJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Bidireccional"
      ],
      "metadata": {
        "id": "rz1kJkkBYnvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNs dominaron el aprendizaje de secuencia-a-secuencia antes de ser superadas por Transformer.\\\n",
        "\n",
        "Los modelos estándar de secuencia-a-secuencia funcionan \"leyendo\" la oración complear antes de traducirla. Esto es importante sobretodo si la tarea trata sobre idiomas, ya que en cada idioma el orden de las palabras es diferente.\n",
        "\n",
        "Una RNN de secuencia-a-secuencia usa un codificador RNN para producir un vector que codifica la secuencia origen completa, que se usa como estado inicial para un decodificador RNN, que observaría los elementos 0...N en la secuencia objetivo e intentaría predecir el paso N+1 en la secuencia objetivo.\n"
      ],
      "metadata": {
        "id": "qqlEml7cCiaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIsRw7N1M9pP",
        "outputId": "1fa9bcdb-6d19-4c80-d741-aabc81972f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = layers.Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "SyTeSXpjZEDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder basado en GRU**\n",
        "\n",
        "GRU solo tiene un vector de estado único, mientras que LSTM tiene varios."
      ],
      "metadata": {
        "id": "QdZ9UZBLpK8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 128\n",
        "\n",
        "# La oración fuente en inglés va aquí. Especificar el nombre de la \n",
        "# entrada nos permite fit() (ajustar) el modelo con un dict de entradas\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "# No olvidemos el enmascaramiento: es fundamental en esta configuración.\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    # Nuestra oración fuente codificada es la \n",
        "    # última salida de un GRU bidireccional.\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ],
      "metadata": {
        "id": "qJ0GqJh_YqQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, agreguamos el decodificador, este se forma con  una capa GRU que toma como estado inicial la oración original codificada, es decir, la salida del codificador.\\\n",
        "Además, agregamos una capa Dense que produce para cada paso de salida una distribución de probabilidad sobre el vocabulario en español."
      ],
      "metadata": {
        "id": "2khFNWKvEjzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# La oración objetivo en español va aquí\n",
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "# No olvidar enmascarar\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "# La oración fuente codificada sirve como el estado \n",
        "# inicial del decodificador GRU.\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "# Predice el siguiente token\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "# Modelo de extremo a extremo: mapea la oración de origen y la oración \n",
        "# de destino a la oración de destino un paso en el futuro\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ],
      "metadata": {
        "id": "Cj6KhvkfY5Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Durante el entrenamiento, el decodificador toma como entrada la secuencia entera, gracias a la naturaleza de los RNN solo tiene en cuenta los tokens de 0 a N para predecir el token N+1 en la salida, esto quiere decir que solo usa la información del pasado para predecir el futuro.\n"
      ],
      "metadata": {
        "id": "iueq8rlYE5Zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entrenamiento**"
      ],
      "metadata": {
        "id": "b6fgu-psFXP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=30, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "7HTTSgBQY9RW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6278e6a0-0bed-4cb3-c892-006a97fdb619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "33/33 [==============================] - 49s 1s/step - loss: 8.9562 - accuracy: 0.0851 - val_loss: 7.1751 - val_accuracy: 0.0933\n",
            "Epoch 2/30\n",
            "33/33 [==============================] - 30s 905ms/step - loss: 6.9127 - accuracy: 0.0894 - val_loss: 6.7278 - val_accuracy: 0.0933\n",
            "Epoch 3/30\n",
            "33/33 [==============================] - 32s 980ms/step - loss: 6.6902 - accuracy: 0.0895 - val_loss: 6.6684 - val_accuracy: 0.0933\n",
            "Epoch 4/30\n",
            "33/33 [==============================] - 32s 978ms/step - loss: 6.6199 - accuracy: 0.0901 - val_loss: 6.6493 - val_accuracy: 0.0933\n",
            "Epoch 5/30\n",
            "33/33 [==============================] - 32s 978ms/step - loss: 6.5808 - accuracy: 0.0914 - val_loss: 6.6362 - val_accuracy: 0.0997\n",
            "Epoch 6/30\n",
            "33/33 [==============================] - 30s 906ms/step - loss: 6.5481 - accuracy: 0.0931 - val_loss: 6.6241 - val_accuracy: 0.0993\n",
            "Epoch 7/30\n",
            "33/33 [==============================] - 32s 979ms/step - loss: 6.5273 - accuracy: 0.0940 - val_loss: 6.6102 - val_accuracy: 0.0976\n",
            "Epoch 8/30\n",
            "33/33 [==============================] - 32s 965ms/step - loss: 6.5024 - accuracy: 0.0944 - val_loss: 6.5984 - val_accuracy: 0.0950\n",
            "Epoch 9/30\n",
            "33/33 [==============================] - 30s 905ms/step - loss: 6.4835 - accuracy: 0.0934 - val_loss: 6.5918 - val_accuracy: 0.0980\n",
            "Epoch 10/30\n",
            "33/33 [==============================] - 38s 1s/step - loss: 6.4762 - accuracy: 0.0935 - val_loss: 6.5890 - val_accuracy: 0.0980\n",
            "Epoch 11/30\n",
            "33/33 [==============================] - 30s 906ms/step - loss: 6.4646 - accuracy: 0.0942 - val_loss: 6.5831 - val_accuracy: 0.0978\n",
            "Epoch 12/30\n",
            "33/33 [==============================] - 30s 908ms/step - loss: 6.4525 - accuracy: 0.0926 - val_loss: 6.5822 - val_accuracy: 0.0980\n",
            "Epoch 13/30\n",
            "33/33 [==============================] - 32s 979ms/step - loss: 6.4474 - accuracy: 0.0927 - val_loss: 6.5773 - val_accuracy: 0.0976\n",
            "Epoch 14/30\n",
            "33/33 [==============================] - 32s 974ms/step - loss: 6.4374 - accuracy: 0.0926 - val_loss: 6.5724 - val_accuracy: 0.0978\n",
            "Epoch 15/30\n",
            "33/33 [==============================] - 32s 972ms/step - loss: 6.4339 - accuracy: 0.0953 - val_loss: 6.5674 - val_accuracy: 0.0984\n",
            "Epoch 16/30\n",
            "33/33 [==============================] - 32s 979ms/step - loss: 6.4213 - accuracy: 0.0950 - val_loss: 6.5594 - val_accuracy: 0.1001\n",
            "Epoch 17/30\n",
            "33/33 [==============================] - 32s 978ms/step - loss: 6.4156 - accuracy: 0.0992 - val_loss: 6.5484 - val_accuracy: 0.1109\n",
            "Epoch 18/30\n",
            "33/33 [==============================] - 32s 980ms/step - loss: 6.4048 - accuracy: 0.1022 - val_loss: 6.5434 - val_accuracy: 0.1144\n",
            "Epoch 19/30\n",
            "33/33 [==============================] - 33s 975ms/step - loss: 6.3883 - accuracy: 0.1053 - val_loss: 6.5290 - val_accuracy: 0.1140\n",
            "Epoch 20/30\n",
            "33/33 [==============================] - 32s 988ms/step - loss: 6.3833 - accuracy: 0.1064 - val_loss: 6.5210 - val_accuracy: 0.1150\n",
            "Epoch 21/30\n",
            "33/33 [==============================] - 32s 981ms/step - loss: 6.3747 - accuracy: 0.1090 - val_loss: 6.5103 - val_accuracy: 0.1205\n",
            "Epoch 22/30\n",
            "33/33 [==============================] - 31s 945ms/step - loss: 6.3635 - accuracy: 0.1076 - val_loss: 6.4986 - val_accuracy: 0.1248\n",
            "Epoch 23/30\n",
            "33/33 [==============================] - 32s 971ms/step - loss: 6.3453 - accuracy: 0.1148 - val_loss: 6.4927 - val_accuracy: 0.1224\n",
            "Epoch 24/30\n",
            "33/33 [==============================] - 32s 978ms/step - loss: 6.3423 - accuracy: 0.1172 - val_loss: 6.4794 - val_accuracy: 0.1341\n",
            "Epoch 25/30\n",
            "33/33 [==============================] - 32s 979ms/step - loss: 6.3361 - accuracy: 0.1200 - val_loss: 6.4713 - val_accuracy: 0.1377\n",
            "Epoch 26/30\n",
            "33/33 [==============================] - 36s 1s/step - loss: 6.3261 - accuracy: 0.1230 - val_loss: 6.4684 - val_accuracy: 0.1388\n",
            "Epoch 27/30\n",
            "33/33 [==============================] - 32s 980ms/step - loss: 6.3093 - accuracy: 0.1243 - val_loss: 6.4619 - val_accuracy: 0.1375\n",
            "Epoch 28/30\n",
            "33/33 [==============================] - 32s 984ms/step - loss: 6.3084 - accuracy: 0.1242 - val_loss: 6.4573 - val_accuracy: 0.1361\n",
            "Epoch 29/30\n",
            "33/33 [==============================] - 36s 1s/step - loss: 6.3011 - accuracy: 0.1256 - val_loss: 6.4503 - val_accuracy: 0.1404\n",
            "Epoch 30/30\n",
            "33/33 [==============================] - 32s 988ms/step - loss: 6.2911 - accuracy: 0.1270 - val_loss: 6.4483 - val_accuracy: 0.1402\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcd6c01900>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se obtienen aún peores resultados, apenas un 14% de precisión, suponemos que al tratar temas diferentes el contexto de una misma palabra en diferentes oraciones cambia, así como las palabras al rededor suya.\\\n",
        "Por lo tanto, al buscar patrones de orden puede suponer una dificultad que haya tan pocas oraciones y además con temas muy diferentes.\n",
        "\n",
        "Nos parece interesante comentar que este modelo no es capaz de aprender de los datos de entrenamiento ni siquiera, los anteriores conseguían una puntuación de precisión relativamente alta para los datos de entrenamiento (entre 0.7 y 0.9), es decir, se ajustaban a estos y aprendían. En cambio este modelo no está sacando características ni patrones de los datos, ya que solo consigue un 10% de precisión durante el entrenamiento."
      ],
      "metadata": {
        "id": "kZc4KupqPrqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Posible solución**"
      ],
      "metadata": {
        "id": "YiP0CP2cPkVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una posible solución para mejorar el rendimiento tanto del Transformer con del RNN Bidireccional sería ampliando el conjunto de datos original.\\\n",
        "Esto podríamos hacerlo aplicando técnicas de Deep Learning Generativo vistas en el curso, es decir, creando un **generador de texto** que reciba como entradas nuestro dataset y genere nuevos textos a partir de este.\\\n",
        "El problema que hemos encontrado al intentar esta propuesta es que al tener tan pocas oraciones de diferentes temas no podemos inicial la generación del texto con un \"promt\" determinado ya que nuestro dataset no trata de un tema en concreto. Por tanto, no tiene sentido indicar un inicio para todos los textos si son de temas diferentes.\\\n",
        "En el caso que vimos de las reseñas de películas es lógico y coherente que todos los textos generados empezaran con \"this movie\", en nuestro caso no es posible.\n",
        "\n",
        "Si tuviesemos muchas más oraciones, aunque fuesen de temas diferentes tendría más sentido aplicar este algoritmo, ya que podríamos obtener resultados interesantes, pero para ello se necesita un dataset mucho más rico.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C_jbOzGuPpse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusiones**"
      ],
      "metadata": {
        "id": "kV5del1bML73"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como hemos podido observar, el Transformer nos ofrece mejores resultados que una RNN bidirecional, esto se debe a que el segundo tiene una capacidad limitada a la hora de capturar patrones de dependencia a largo plazo.\\\n",
        "En una RNN bidireccional la información transcurre en ambos sentidos simultaneamente, lo que permite capturar cierta información del contexto de las palabras cercanas a la palabra actual. En cambio, la dependencia a largo plazo es más dificil de capturar debido al **desvanecimiento del gradiente**, esto se produce cuando se propagan gradientes a través de una gran cantidad de pasos de tiempo, lo que hace que la información sea cada vez más dificil de recuperar.\n",
        "\n",
        "Sin embargo, el Transformer se basa en la multihead attention para capturar estos patrones de dependencia a largo plazo. Esto le permite capturar mejor la relación entre palabras distantes, y consecuentemente, ofrece un mejor rendimiento en este tipo de tareas.\n",
        "\n",
        "Además, el Transformer puede procesar todos los elementos de una secuencia simultaneamente, mientras que en un RNN, la información debe procesarse en orden secuencial, lo que lo hace menos eficiente.\n",
        "\n"
      ],
      "metadata": {
        "id": "GRvXZSEJKgj4"
      }
    }
  ]
}